{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- アンサンブルを行ってみる\n",
    "- stackingを行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = '091'\n",
    "isSmallSet = False\n",
    "if isSmallSet:\n",
    "    LENGTH = 7000\n",
    "else:\n",
    "    LENGTH = 500_000\n",
    "\n",
    "PATH_TRAIN = './../data/input/train_clean.csv'\n",
    "PATH_TEST = './../data/input/test_clean.csv'\n",
    "PATH_SMPLE_SUB = './../data/input/sample_submission.csv'\n",
    "DIR_OUTPUT = './../data/output/'\n",
    "DIR_OUTPUT_IGNORE = './../data/output_ignore/'\n",
    "cp = ['#f8b195', '#f67280', '#c06c84', '#6c5b7b', '#355c7d']\n",
    "sr = 10*10**3  # 10 kHz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import everything I need :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import glob\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "# from pykalman import KalmanFilter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from fastprogress import progress_bar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import f1_score, mean_absolute_error, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_macro(true, pred):\n",
    "    return f1_score(true, pred, average='macro')\n",
    "\n",
    "def get_df_batch(df, batch):\n",
    "    idxs = df['batch'] == batch\n",
    "    assert any(idxs), 'そのようなbatchはありません'\n",
    "    return df[idxs]\n",
    "    \n",
    "def get_signal_mv_mean(df, n=3001):\n",
    "    signal_mv = np.zeros(len(df))\n",
    "    for bt in df['batch'].unique():\n",
    "        idxs = df['batch'] == bt\n",
    "        _signal_mv = df['signal'][idxs].rolling(n, center=True).mean().interpolate('spline', order=5, limit_direction='both').values\n",
    "        signal_mv[idxs] = _signal_mv\n",
    "    return signal_mv\n",
    "\n",
    "def get_signal_mv_std(df, n=3001):\n",
    "    signal_mv = np.zeros(len(df))\n",
    "    for bt in df['batch'].unique():\n",
    "        idxs = df['batch'] == bt\n",
    "        _signal_mv = df['signal'][idxs].rolling(n, center=True).std().interpolate('spline', order=5, limit_direction='both').values\n",
    "        signal_mv[idxs] = _signal_mv\n",
    "    return signal_mv\n",
    "\n",
    "def get_signal_mv_min(df, n=3001):\n",
    "    signal_mv = np.zeros(len(df))\n",
    "    for bt in df['batch'].unique():\n",
    "        idxs = df['batch'] == bt\n",
    "        _signal_mv = df['signal'][idxs].rolling(n, center=True).min().interpolate('spline', order=5, limit_direction='both').values\n",
    "        signal_mv[idxs] = _signal_mv\n",
    "    return signal_mv\n",
    "\n",
    "def get_signal_mv_max(df, n=3001):\n",
    "    signal_mv = np.zeros(len(df))\n",
    "    for bt in df['batch'].unique():\n",
    "        idxs = df['batch'] == bt\n",
    "        _signal_mv = df['signal'][idxs].rolling(n, center=True).max().interpolate('spline', order=5, limit_direction='both').values\n",
    "        signal_mv[idxs] = _signal_mv\n",
    "    return signal_mv\n",
    "\n",
    "\n",
    "\n",
    "def group_feat_train(_train):\n",
    "    train = _train.copy()\n",
    "    # group init\n",
    "    train['group'] = int(0)\n",
    "\n",
    "    # group 1\n",
    "    idxs = (train['batch'] == 3) | (train['batch'] == 7)\n",
    "    train['group'][idxs] = int(1)\n",
    "\n",
    "    # group 2\n",
    "    idxs = (train['batch'] == 5) | (train['batch'] == 8)\n",
    "    train['group'][idxs] = int(2)\n",
    "\n",
    "    # group 3\n",
    "    idxs = (train['batch'] == 2) | (train['batch'] == 6)\n",
    "    train['group'][idxs] = int(3)\n",
    "\n",
    "    # group 4\n",
    "    idxs = (train['batch'] == 4) | (train['batch'] == 9)\n",
    "    train['group'][idxs] = int(4)\n",
    "    \n",
    "    return train[['group']]\n",
    "\n",
    "def group_feat_test(_test):\n",
    "    test = _test.copy()\n",
    "    \n",
    "    # group init\n",
    "    test['group'] = int(0)\n",
    "    x_idx = np.arange(len(test))\n",
    "\n",
    "    # group 1\n",
    "    idxs = (100000<=x_idx) & (x_idx<200000)\n",
    "    test['group'][idxs] = int(1)\n",
    "    idxs = (900000<=x_idx) & (x_idx<=1000000)\n",
    "    test['group'][idxs] = int(1)\n",
    "\n",
    "    # group 2\n",
    "    idxs = (200000<=x_idx) & (x_idx<300000)\n",
    "    test['group'][idxs] = int(2)\n",
    "    idxs = (600000<=x_idx) & (x_idx<700000)\n",
    "    test['group'][idxs] = int(2)\n",
    "\n",
    "    # group 3\n",
    "    idxs = (400000<=x_idx) & (x_idx<500000)\n",
    "    test['group'][idxs] = int(3)\n",
    "\n",
    "    # group 4\n",
    "    idxs = (500000<=x_idx) & (x_idx<600000)\n",
    "    test['group'][idxs] = int(4)\n",
    "    idxs = (700000<=x_idx) & (x_idx<800000)\n",
    "    test['group'][idxs] = int(4)\n",
    "    \n",
    "    return test[['group']]\n",
    "\n",
    "\n",
    "class permutation_importance():\n",
    "    def __init__(self, model, metric):\n",
    "        self.is_computed = False\n",
    "        self.n_feat = 0\n",
    "        self.base_score = 0\n",
    "        self.model = model\n",
    "        self.metric = metric\n",
    "        self.df_result = []\n",
    "    \n",
    "    def compute(self, X_valid, y_valid):\n",
    "        self.n_feat = len(X_valid.columns)\n",
    "        if self.metric == 'auc':\n",
    "            y_valid_score = self.model.predict_proba(X_valid)[:, 1]\n",
    "            fpr, tpr, thresholds = roc_curve(y_valid, y_valid_score)\n",
    "            self.base_score = auc(fpr, tpr)\n",
    "        else:\n",
    "            pred = np.round(self.model.predict(X_valid)).astype('int8')\n",
    "            self.base_score = self.metric(y_valid, pred)\n",
    "        self.df_result = pd.DataFrame({'feat': X_valid.columns, \n",
    "                                       'score': np.zeros(self.n_feat),\n",
    "                                       'score_diff': np.zeros(self.n_feat)})\n",
    "        \n",
    "        # predict\n",
    "        for i, col in enumerate(X_valid.columns):\n",
    "            df_perm = X_valid.copy()\n",
    "            np.random.seed(1)\n",
    "            df_perm[col] = np.random.permutation(df_perm[col])\n",
    "            y_valid_pred = self.model.predict(df_perm)\n",
    "            if self.metric == 'auc':\n",
    "                y_valid_score = self.model.predict_proba(df_perm)[:, 1]\n",
    "                fpr, tpr, thresholds = roc_curve(y_valid, y_valid_score)\n",
    "                score = auc(fpr, tpr)\n",
    "            else:\n",
    "                score = self.metric(y_valid, np.round(y_valid_pred).astype('int8'))\n",
    "            self.df_result['score'][self.df_result['feat']==col] = score\n",
    "            self.df_result['score_diff'][self.df_result['feat']==col] = self.base_score - score\n",
    "        self.is_computed = True\n",
    "    \n",
    "    def get_negative_feature(self):\n",
    "        assert self.is_computed!=False, 'compute メソッドが実行されていません'\n",
    "        idx = self.df_result['score_diff'] < 0\n",
    "        return self.df_result.loc[idx, 'feat'].values.tolist()\n",
    "        \n",
    "    def get_positive_feature(self):\n",
    "        assert self.is_computed!=False, 'compute メソッドが実行されていません'\n",
    "        idx = self.df_result['score_diff'] > 0\n",
    "        return self.df_result.loc[idx, 'feat'].values.tolist()\n",
    "    \n",
    "    def show_permutation_importance(self, score_type='loss'):\n",
    "        '''score_type = 'loss' or 'accuracy'  '''\n",
    "        assert self.is_computed!=False, 'compute メソッドが実行されていません'\n",
    "        if score_type=='loss':\n",
    "            ascending = True\n",
    "        elif score_type=='accuracy':\n",
    "            ascending = False\n",
    "        else:\n",
    "            ascending = ''\n",
    "        \n",
    "        plt.figure(figsize=(15, int(0.25*self.n_feat)))\n",
    "        sns.barplot(x=\"score_diff\", y=\"feat\", data=self.df_result.sort_values(by=\"score_diff\", ascending=ascending))\n",
    "        plt.title('base_score - permutation_score')\n",
    "\n",
    "def plot_corr(df, abs_=False, threshold=0.95):\n",
    "    if abs_==True:\n",
    "        corr = df.corr().abs()>threshold\n",
    "        vmin = 0\n",
    "    else:\n",
    "        corr = df.corr()\n",
    "        vmin = -1\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 10), dpi=100)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    sns.heatmap(corr,\n",
    "                xticklabels=df.corr().columns,\n",
    "                yticklabels=df.corr().columns,\n",
    "                vmin=vmin,\n",
    "                vmax=1,\n",
    "                center=0, \n",
    "                annot=False)\n",
    "\n",
    "    # Decorations\n",
    "    ax.set_title('Correlation', fontsize=22)\n",
    "\n",
    "def get_low_corr_column(df, threshold):\n",
    "\n",
    "    df_corr = df.corr()\n",
    "    df_corr = abs(df_corr)\n",
    "    columns = df_corr.columns\n",
    "\n",
    "    # 対角線の値を0にする\n",
    "    for i in range(0, len(columns)):\n",
    "        df_corr.iloc[i, i] = 0\n",
    "\n",
    "    while True:\n",
    "        columns = df_corr.columns\n",
    "        max_corr = 0.0\n",
    "        query_column = None\n",
    "        target_column = None\n",
    "\n",
    "        df_max_column_value = df_corr.max()\n",
    "        max_corr = df_max_column_value.max()\n",
    "        query_column = df_max_column_value.idxmax()\n",
    "        target_column = df_corr[query_column].idxmax()\n",
    "\n",
    "        if max_corr < threshold:\n",
    "            # しきい値を超えるものがなかったため終了\n",
    "            break\n",
    "        else:\n",
    "            # しきい値を超えるものがあった場合\n",
    "            delete_column = None\n",
    "            saved_column = None\n",
    "\n",
    "            # その他との相関の絶対値が大きい方を除去\n",
    "            if sum(df_corr[query_column]) <= sum(df_corr[target_column]):\n",
    "                delete_column = target_column\n",
    "                saved_column = query_column\n",
    "            else:\n",
    "                delete_column = query_column\n",
    "                saved_column = target_column\n",
    "\n",
    "            # 除去すべき特徴を相関行列から消す（行、列）\n",
    "            df_corr.drop([delete_column], axis=0, inplace=True)\n",
    "            df_corr.drop([delete_column], axis=1, inplace=True)\n",
    "\n",
    "    return df_corr.columns  # 相関が高い特徴量を除いた名前リスト\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        if col!='open_channels':\n",
    "            col_type = df[col].dtypes\n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)  \n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def create_signal_mod(train):\n",
    "    left = 3641000\n",
    "    right = 3829000\n",
    "    thresh_dict = {\n",
    "        3: [0.1, 2.0],\n",
    "        2: [-1.1, 0.7],\n",
    "        1: [-2.3, -0.6],\n",
    "        0: [-3.8, -2],\n",
    "    }\n",
    "    \n",
    "    train['signal'] = train['signal'].values\n",
    "    for ch in train[train['batch']==7]['open_channels'].unique():\n",
    "        idxs_noisy = (train['open_channels']==ch) & (left<train.index) & (train.index<right)\n",
    "        idxs_not_noisy = (train['open_channels']==ch) & ~idxs_noisy\n",
    "        mean = train[idxs_not_noisy]['signal'].mean()\n",
    "\n",
    "        idxs_outlier = idxs_noisy & (thresh_dict[ch][1]<train['signal'].values)\n",
    "        train['signal'][idxs_outlier]  = mean\n",
    "        idxs_outlier = idxs_noisy & (train['signal'].values<thresh_dict[ch][0])\n",
    "        train['signal'][idxs_outlier]  = mean\n",
    "    return train\n",
    "\n",
    "def create_signal_mod2(train):\n",
    "    left = 3641000\n",
    "    right = 3829000\n",
    "    thresh_dict = {\n",
    "        3: [0.1, 2.0],\n",
    "        2: [-1.1, 0.7],\n",
    "        1: [-2.3, -0.6],\n",
    "        0: [-3.8, -2],\n",
    "    }\n",
    "    \n",
    "    train['signal'] = train['signal'].values\n",
    "    for ch in train[train['batch']==7]['open_channels'].unique():\n",
    "        idxs_noisy = (train['open_channels']==ch) & (left<train.index) & (train.index<right)\n",
    "        idxs_not_noisy = (train['open_channels']==ch) & ~idxs_noisy\n",
    "        mean = train[idxs_not_noisy]['signal'].mean()\n",
    "        std = train[idxs_not_noisy]['signal'].std()\n",
    "\n",
    "        idxs_outlier = idxs_noisy & (thresh_dict[ch][1]<train['signal'].values)\n",
    "        noise = np.random.normal(loc=0, scale=std, size=len(train['signal'].values[idxs_outlier]))\n",
    "        train['signal'][idxs_outlier]  = mean + noise\n",
    "        idxs_outlier = idxs_noisy & (train['signal'].values<thresh_dict[ch][0])\n",
    "        noise = np.random.normal(loc=0, scale=std, size=len(train['signal'].values[idxs_outlier]))\n",
    "        train['signal'][idxs_outlier]  = mean + noise\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(X, y, X_te, lgbm_params, random_state=5, n_fold=5, verbose=50, early_stopping_rounds=100, show_fig=True):\n",
    "    # using features\n",
    "    print(f'features({len(X.columns)}): \\n{X.columns}') if not verbose==0 else None\n",
    "\n",
    "#     folds = KFold(n_splits=n_fold, shuffle=True, random_state=random_state)\n",
    "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=random_state)\n",
    "\n",
    "    scores = []\n",
    "    oof = np.zeros(len(X))\n",
    "    oof_round = np.zeros(len(X))\n",
    "    test_pred = np.zeros(len(X_te))\n",
    "    df_pi = pd.DataFrame(columns=['feat', 'score_diff'])\n",
    "    for fold_n, (train_idx, valid_idx) in enumerate(folds.split(X, y=y)):\n",
    "        if  verbose==0:\n",
    "            pass\n",
    "        else:\n",
    "            print('\\n------------------')\n",
    "            print(f'- Fold {fold_n + 1}/{N_FOLD} started at {time.ctime()}')\n",
    "\n",
    "        # prepare dataset\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "\n",
    "        # train\n",
    "        model = LGBMRegressor(**lgbm_params, n_estimators=N_ESTIMATORS)\n",
    "        model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                  verbose=verbose,\n",
    "                  early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "        # pred\n",
    "        y_valid_pred = model.predict(X_valid, model.best_iteration_)\n",
    "        y_valid_pred_round = np.round(y_valid_pred).astype('int8')\n",
    "        _test_pred = model.predict(X_te, model.best_iteration_)\n",
    "\n",
    "        if show_fig==False:\n",
    "            pass\n",
    "        else:\n",
    "            # permutation importance\n",
    "            pi = permutation_importance(model, f1_macro) # model と metric を渡す\n",
    "            pi.compute(X_valid, y_valid)\n",
    "            pi_result = pi.df_result\n",
    "            df_pi = pd.concat([df_pi, pi_result[['feat', 'score_diff']]])\n",
    "\n",
    "        # result\n",
    "        oof[valid_idx] = y_valid_pred\n",
    "        oof_round[valid_idx] = y_valid_pred_round\n",
    "        score = f1_score(y_valid, y_valid_pred_round, average='macro')\n",
    "        scores.append(score)\n",
    "        test_pred += _test_pred\n",
    "        if verbose==0:\n",
    "            pass\n",
    "        else:\n",
    "            print(f'---> f1-score(macro) valid: {f1_score(y_valid, y_valid_pred_round, average=\"macro\"):.4f}')\n",
    "            print('')\n",
    "\n",
    "\n",
    "    print('====== finish ======')\n",
    "    print('score list:', scores)\n",
    "    print('CV mean score(f1_macro): {0:.4f}, std: {1:.4f}'.format(np.mean(scores), np.std(scores)))\n",
    "    print(f'oof score(f1_macro): {f1_score(y, oof_round, average=\"macro\"):.4f}')\n",
    "    print('')\n",
    "\n",
    "\n",
    "    if show_fig==False:\n",
    "        pass\n",
    "    else:\n",
    "        # visualization\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.plot([0, 10], [0, 10], color='gray')\n",
    "        plt.scatter(y, oof, alpha=0.05, color=cp[1])\n",
    "        plt.xlabel('true')\n",
    "        plt.ylabel('pred')\n",
    "        plt.show()\n",
    "          \n",
    "        # confusion_matrix\n",
    "        plot_confusion_matrix(y, oof_round, classes=np.arange(11))\n",
    "        \n",
    "        \n",
    "        # permutation importance\n",
    "        plt.figure(figsize=(15, int(0.25*len(X.columns))))\n",
    "        order = df_pi.groupby([\"feat\"]).mean()['score_diff'].reset_index().sort_values('score_diff', ascending=False)\n",
    "        sns.barplot(x=\"score_diff\", y=\"feat\", data=df_pi, order=order['feat'])\n",
    "        plt.title('base_score - permutation_score')\n",
    "        plt.show()\n",
    "\n",
    "    # submission\n",
    "    test_pred = test_pred/N_FOLD\n",
    "    test_pred_round = np.round(test_pred).astype('int8')\n",
    "      \n",
    "    return test_pred_round, test_pred, oof_round, oof\n",
    "\n",
    "def plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n",
    "    cm = confusion_matrix(truth, pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix', size=15)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_lgbm(X, y, X_te, lgbm_params, random_state=5, test_size=0.3, verbose=50, early_stopping_rounds=100, show_fig=True):\n",
    "    # using features\n",
    "    print(f'features({len(X.columns)}): \\n{X.columns}') if not verbose==0 else None\n",
    "\n",
    "#     folds = KFold(n_splits=n_fold, shuffle=True, random_state=random_state)\n",
    "#     folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # prepare dataset\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # train\n",
    "    model = LGBMRegressor(**lgbm_params, n_estimators=N_ESTIMATORS)\n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "              verbose=verbose,\n",
    "              early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "    # pred\n",
    "    oof = model.predict(X_valid, model.best_iteration_)\n",
    "    oof_round = np.round(oof).astype('int8')\n",
    "    test_pred = model.predict(X_te, model.best_iteration_)\n",
    "    test_pred_round = np.round(test_pred).astype('int8')\n",
    "\n",
    "    print('====== finish ======')\n",
    "    print(f'oof score(f1_macro): {f1_score(y_valid, oof_round, average=\"macro\"):.4f}')\n",
    "    print('')\n",
    "\n",
    "\n",
    "    if show_fig==False:\n",
    "        pass\n",
    "    else:\n",
    "        # visualization\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.plot([0, 10], [0, 10], color='gray')\n",
    "        plt.scatter(y_valid, oof, alpha=0.05, color=cp[1])\n",
    "        plt.xlabel('true')\n",
    "        plt.ylabel('pred')\n",
    "        plt.show()\n",
    "          \n",
    "        # confusion_matrix\n",
    "        plot_confusion_matrix(y_valid, oof_round, classes=np.arange(11))\n",
    "        \n",
    "        # permutation importance\n",
    "        pi = permutation_importance(model, f1_macro) # model と metric を渡す\n",
    "        pi.compute(X_valid, y_valid)\n",
    "        pi.show_permutation_importance(score_type='accuracy')  # loss or accuracy\n",
    "        plt.show()\n",
    "\n",
    "    return test_pred_round, test_pred, oof_round, oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm_clf(X, y, X_te, lgbm_params, random_state=5, n_fold=5, verbose=50, early_stopping_rounds=100, show_fig=True):\n",
    "    # using features\n",
    "    print(f'features({len(X.columns)}): \\n{X.columns}') if not verbose==0 else None\n",
    "\n",
    "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=random_state)\n",
    "\n",
    "    scores = []\n",
    "    oof_proba = np.zeros([len(X), len(np.unique(y))])\n",
    "    test_proba = np.zeros([len(X_te), len(np.unique(y))])\n",
    "    df_pi = pd.DataFrame(columns=['feat', 'score_diff'])\n",
    "    for fold_n, (train_idx, valid_idx) in enumerate(folds.split(X, y=y)):\n",
    "        if  verbose==0:\n",
    "            pass\n",
    "        else:\n",
    "            print('\\n------------------')\n",
    "            print(f'- Fold {fold_n + 1}/{N_FOLD} started at {time.ctime()}')\n",
    "\n",
    "        # prepare dataset\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "\n",
    "        # train\n",
    "        model = LGBMClassifier(**lgbm_params)\n",
    "        model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                  verbose=verbose,\n",
    "                  early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "        # pred\n",
    "#         y_valid_pred = model.predict(X_valid, model.best_iteration_)\n",
    "        y_valid_proba = model.predict_proba(X_valid, num_iteration=model.best_iteration_)\n",
    "        y_valid_pred = np.argmax(y_valid_proba, axis=1)\n",
    "#         _test_pred = model.predict(X_te, model.best_iteration_)\n",
    "        _test_proba = model.predict_proba(X_te, num_iteration=model.best_iteration_)\n",
    "        _test_pred = np.argmax(_test_proba, axis=1)\n",
    "\n",
    "        if show_fig==False:\n",
    "            pass\n",
    "        else:\n",
    "            # permutation importance\n",
    "            pi = permutation_importance(model, f1_macro) # model と metric を渡す\n",
    "            pi.compute(X_valid, y_valid)\n",
    "            pi_result = pi.df_result\n",
    "            df_pi = pd.concat([df_pi, pi_result[['feat', 'score_diff']]])\n",
    "\n",
    "        # result\n",
    "        oof_proba[valid_idx] = y_valid_proba\n",
    "        score = f1_score(y_valid, y_valid_pred, average='macro')\n",
    "        scores.append(score)\n",
    "        test_proba += _test_proba\n",
    "        if verbose==0:\n",
    "            pass\n",
    "        else:\n",
    "            print(f'---> f1-score(macro) valid: {f1_score(y_valid, y_valid_pred, average=\"macro\"):.4f}')\n",
    "            print('')\n",
    "\n",
    "\n",
    "    print('====== finish ======')\n",
    "    oof = np.argmax(oof_proba, axis=1)\n",
    "    print('score list:', scores)\n",
    "    print('CV mean score(f1_macro): {0:.4f}, std: {1:.4f}'.format(np.mean(scores), np.std(scores)))\n",
    "    print(f'oof score(f1_macro): {f1_score(y, oof, average=\"macro\"):.4f}')\n",
    "    print('')\n",
    "\n",
    "\n",
    "    if show_fig==False:\n",
    "        pass\n",
    "    else:\n",
    "        # visualization\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.plot([0, 10], [0, 10], color='gray')\n",
    "        plt.scatter(y, oof, alpha=0.05, color=cp[1])\n",
    "        plt.xlabel('true')\n",
    "        plt.ylabel('pred')\n",
    "        plt.show()\n",
    "          \n",
    "        # confusion_matrix\n",
    "        plot_confusion_matrix(y, oof, classes=np.arange(11))\n",
    "        \n",
    "        \n",
    "        # permutation importance\n",
    "        plt.figure(figsize=(15, int(0.25*len(X.columns))))\n",
    "        order = df_pi.groupby([\"feat\"]).mean()['score_diff'].reset_index().sort_values('score_diff', ascending=False)\n",
    "        sns.barplot(x=\"score_diff\", y=\"feat\", data=df_pi, order=order['feat'])\n",
    "        plt.title('base_score - permutation_score')\n",
    "        plt.show()\n",
    "\n",
    "    # submission\n",
    "    test_proba = test_proba/N_FOLD \n",
    "    test_pred = np.argmax(test_proba, axis=1)\n",
    "#     oof_pred = np.argmax(oof_proba, axis=1)\n",
    "      \n",
    "    return test_pred, test_proba, oof, oof_proba, type(model).__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "ref: https://www.kaggle.com/martxelo/fe-and-ensemble-mlp-and-lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradients(s, n_grads=4):\n",
    "    '''\n",
    "    Calculate gradients for a pandas series. Returns the same number of samples\n",
    "    '''\n",
    "    grads = pd.DataFrame()\n",
    "    \n",
    "    g = s.values\n",
    "    for i in range(n_grads):\n",
    "        g = np.gradient(g)\n",
    "        grads['grad_' + str(i+1)] = g\n",
    "        \n",
    "    return grads\n",
    "\n",
    "\n",
    "def calc_low_pass(s, n_filts=10):\n",
    "    '''\n",
    "    Applies low pass filters to the signal. Left delayed and no delayed\n",
    "    '''\n",
    "    wns = np.logspace(-2, -0.3, n_filts)\n",
    "#     wns = [0.3244]\n",
    "    \n",
    "    low_pass = pd.DataFrame()\n",
    "    x = s.values\n",
    "    for wn in wns:\n",
    "        b, a = signal.butter(1, Wn=wn, btype='low')\n",
    "        zi = signal.lfilter_zi(b, a)\n",
    "        low_pass['lowpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n",
    "        low_pass['lowpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n",
    "        \n",
    "    return low_pass\n",
    "\n",
    "def calc_high_pass(s, n_filts=10):\n",
    "    '''\n",
    "    Applies high pass filters to the signal. Left delayed and no delayed\n",
    "    '''\n",
    "    wns = np.logspace(-2, -0.1, n_filts)\n",
    "#     wns = [0.0100, 0.0264, 0.0699, 0.3005, 0.4885, 0.7943]\n",
    "    \n",
    "    high_pass = pd.DataFrame()\n",
    "    x = s.values\n",
    "    for wn in wns:\n",
    "        b, a = signal.butter(1, Wn=wn, btype='high')\n",
    "        zi = signal.lfilter_zi(b, a)\n",
    "        high_pass['highpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n",
    "        high_pass['highpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n",
    "        \n",
    "    return high_pass\n",
    "\n",
    "def calc_roll_stats(s, windows=[10, 50, 100, 500, 1000, 3000]):\n",
    "    '''\n",
    "    Calculates rolling stats like mean, std, min, max...\n",
    "    '''\n",
    "    roll_stats = pd.DataFrame()\n",
    "    for w in windows:\n",
    "        roll_stats['roll_mean_' + str(w)] = s.rolling(window=w, min_periods=1).mean().interpolate('spline', order=5, limit_direction='both')\n",
    "        roll_stats['roll_std_' + str(w)] = s.rolling(window=w, min_periods=1).std().interpolate('spline', order=5, limit_direction='both')\n",
    "        roll_stats['roll_min_' + str(w)] = s.rolling(window=w, min_periods=1).min().interpolate('spline', order=5, limit_direction='both')\n",
    "        roll_stats['roll_max_' + str(w)] = s.rolling(window=w, min_periods=1).max().interpolate('spline', order=5, limit_direction='both')\n",
    "        roll_stats['roll_range_' + str(w)] = roll_stats['roll_max_' + str(w)] - roll_stats['roll_min_' + str(w)]\n",
    "        roll_stats['roll_q10_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.10).interpolate('spline', order=5, limit_direction='both')\n",
    "        roll_stats['roll_q25_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.25).interpolate('spline', order=5, limit_direction='both')\n",
    "        roll_stats['roll_q50_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.50).interpolate('spline', order=5, limit_direction='both')\n",
    "        roll_stats['roll_q75_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.75).interpolate('spline', order=5, limit_direction='both')\n",
    "        roll_stats['roll_q90_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.90).interpolate('spline', order=5, limit_direction='both')\n",
    "    \n",
    "    # add zeros when na values (std)\n",
    "#     roll_stats = roll_stats.fillna(value=0)\n",
    "             \n",
    "    return roll_stats\n",
    "\n",
    "def calc_ewm(s, windows=[10, 50, 100, 500, 1000, 3000]):\n",
    "    '''\n",
    "    Calculates exponential weighted functions\n",
    "    '''\n",
    "    ewm = pd.DataFrame()\n",
    "    for w in windows:\n",
    "        ewm['ewm_mean_' + str(w)] = s.ewm(span=w, min_periods=1).mean()\n",
    "        ewm['ewm_std_' + str(w)] = s.ewm(span=w, min_periods=1).std()\n",
    "        \n",
    "    # add zeros when na values (std)\n",
    "    ewm = ewm.fillna(value=0)\n",
    "        \n",
    "    return ewm\n",
    "\n",
    "\n",
    "\n",
    "def divide_and_add_features(s, signal_size=500000):\n",
    "    '''\n",
    "    Divide the signal in bags of \"signal_size\".\n",
    "    Normalize the data dividing it by 15.0\n",
    "    '''\n",
    "    # normalize\n",
    "    s = s/15.0\n",
    "    \n",
    "    ls = []\n",
    "    for i in progress_bar(range(int(s.shape[0]/signal_size))):\n",
    "        sig = s[i*signal_size:(i+1)*signal_size].copy().reset_index(drop=True)\n",
    "        sig_featured = add_features(sig)\n",
    "        ls.append(sig_featured)\n",
    "    \n",
    "    return pd.concat(ls, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "ref: https://www.kaggle.com/nxrprime/single-model-lgbm-kalman-filter-ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kalman1D(observations,damping=1):\n",
    "    # To return the smoothed time series data\n",
    "    observation_covariance = damping\n",
    "    initial_value_guess = observations[0]\n",
    "    transition_matrix = 1\n",
    "    transition_covariance = 0.1\n",
    "    initial_value_guess\n",
    "    kf = KalmanFilter(\n",
    "            initial_state_mean=initial_value_guess,\n",
    "            initial_state_covariance=observation_covariance,\n",
    "            observation_covariance=observation_covariance,\n",
    "            transition_covariance=transition_covariance,\n",
    "            transition_matrices=transition_matrix\n",
    "        )\n",
    "    pred_state, state_cov = kf.smooth(observations)\n",
    "    return pred_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = pd.read_csv(PATH_TRAIN)\n",
    "y = df_tr['open_channels'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "notebook list(アンサンブルに使うノートブック番号)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_list = [\n",
    "    # ensemble用\n",
    "    71, 74, \n",
    "    86, 87,\n",
    "    67, 72, \n",
    "    73, 77, \n",
    "    75, 78,\n",
    "    83, 84,\n",
    "    \n",
    "    # その他で、LB(pub)が0.945を超えているもの\n",
    "    63, 56, 46\n",
    "    \n",
    "#     59, 66\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_all_paths = glob.glob('./../data/output_ignore/*.npz')\n",
    "\n",
    "probas_paths = []\n",
    "for nb in nb_list:\n",
    "    for path in probas_all_paths:\n",
    "        probas_paths.append(path) if str(nb).zfill(3) in path else None\n",
    "assert len(probas_paths)==len(nb_list), 'nb_listとprobas_pathsの数が一致しません'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./../data/output_ignore/probas_nb071_cv_0.9409.npz',\n",
       " './../data/output_ignore/probas_nb074_cv_0.9407.npz',\n",
       " './../data/output_ignore/probas_nb086_cv_0.9407.npz',\n",
       " './../data/output_ignore/probas_nb087_cv_0.9406.npz',\n",
       " './../data/output_ignore/probas_nb067_cv_0.9410.npz',\n",
       " './../data/output_ignore/probas_nb072_cv_0.9405.npz',\n",
       " './../data/output_ignore/probas_nb073_cv_0.9410.npz',\n",
       " './../data/output_ignore/probas_nb077_cv_0.9413.npz',\n",
       " './../data/output_ignore/probas_nb075_cv_0.9413.npz',\n",
       " './../data/output_ignore/probas_nb078_cv_0.9410.npz',\n",
       " './../data/output_ignore/probas_nb083_cv_0.9400.npz',\n",
       " './../data/output_ignore/probas_nb084_cv_0.9409.npz',\n",
       " './../data/output_ignore/probas_nb063_cv_0.9416.npz',\n",
       " './../data/output_ignore/probas_nb056_cv_0.9416.npz',\n",
       " './../data/output_ignore/probas_nb046_cv_0.9411.npz']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensenble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='15' class='' max='15', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [15/15 00:51<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# oof_proba_ave = np.zeros([5000000, 11])\n",
    "# sub_proba_ave = np.zeros([2000000, 11])\n",
    "dict_oof_proba = {}\n",
    "dict_sub_proba = {}\n",
    "for i_path, path in enumerate(progress_bar(probas_paths)):\n",
    "    probas = np.load(path)\n",
    "    \n",
    "    # oof and sub\n",
    "    oof_proba = probas['arr_0']\n",
    "    sub_proba = probas['arr_1']\n",
    "    dict_oof_proba[str(nb_list[i_path])] = oof_proba.astype('float32')\n",
    "    dict_sub_proba[str(nb_list[i_path])] = sub_proba.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "dfs_te = []\n",
    "for key in dict_oof_proba.keys():\n",
    "    cols = [f'{key}_proba_{i}' for i in range(11)]\n",
    "    df = pd.DataFrame(dict_oof_proba[key], columns=cols)\n",
    "    dfs.append(df)\n",
    "    \n",
    "    df_te = pd.DataFrame(dict_sub_proba[key], columns=cols)\n",
    "    dfs_te.append(df_te)\n",
    "X = pd.concat(dfs, axis=1)\n",
    "X_te = pd.concat(dfs_te, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000000, 165), (2000000, 165))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "_X = X[::1000].copy() if debug else X.copy()\n",
    "_y = y[::1000].copy() if debug else y.copy()\n",
    "_X_te = X_te[::1000].copy() if debug else X_te.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "N_ESTIMATORS = 2000\n",
    "# N_ESTIMATORS = 20          # 最大学習回数\n",
    "VERBOSE = 100               # 300回ごとに評価する\n",
    "EARLY_STOPPING_ROUNDS = 50  # 200回の学習でよくならなければ、学習をとめる\n",
    "# N_JOBS = multiprocessing.cpu_count() - 2\n",
    "# N_JOBS = 6\n",
    "# N_FOLD = 4\n",
    "# KFOLD_SEED = 0\n",
    "\n",
    "N_JOBS = 33\n",
    "N_FOLD = 6\n",
    "KFOLD_SEED = 42\n",
    "\n",
    "# https://www.kaggle.com/nxrprime/single-model-lgbm-kalman-filter-ii\n",
    "lgbm_params = {'boosting_type': 'gbdt',\n",
    "          'objective': 'multiclass',\n",
    "#           'metric': 'rmse',\n",
    "          'num_class': 11,\n",
    "          'n_jobs': N_JOBS,\n",
    "          'seed': 236,\n",
    "          'n_estimators': N_ESTIMATORS,\n",
    "          'num_leaves': 280,\n",
    "          'learning_rate': 0.03,\n",
    "          'max_depth': 73,\n",
    "          'lambda_l1': 2.959759088169741,\n",
    "          'lambda_l2': 1.331172832164913,\n",
    "          'bagging_fraction': 0.9655406551472153,\n",
    "          'bagging_freq': 9,\n",
    "          'colsample_bytree': 0.6867118652742716\n",
    "              }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features(165): \n",
      "Index(['71_proba_0', '71_proba_1', '71_proba_2', '71_proba_3', '71_proba_4',\n",
      "       '71_proba_5', '71_proba_6', '71_proba_7', '71_proba_8', '71_proba_9',\n",
      "       ...\n",
      "       '46_proba_1', '46_proba_2', '46_proba_3', '46_proba_4', '46_proba_5',\n",
      "       '46_proba_6', '46_proba_7', '46_proba_8', '46_proba_9', '46_proba_10'],\n",
      "      dtype='object', length=165)\n",
      "\n",
      "------------------\n",
      "- Fold 1/6 started at Tue May 19 21:13:15 2020\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's multi_logloss: 0.138543\tvalid_1's multi_logloss: 0.148188\n",
      "[200]\ttraining's multi_logloss: 0.0618421\tvalid_1's multi_logloss: 0.0801311\n",
      "[300]\ttraining's multi_logloss: 0.0497541\tvalid_1's multi_logloss: 0.0747726\n",
      "[400]\ttraining's multi_logloss: 0.0439275\tvalid_1's multi_logloss: 0.0741388\n",
      "Early stopping, best iteration is:\n",
      "[422]\ttraining's multi_logloss: 0.0429147\tvalid_1's multi_logloss: 0.0741254\n",
      "---> f1-score(macro) valid: 0.9426\n",
      "\n",
      "\n",
      "------------------\n",
      "- Fold 2/6 started at Tue May 19 21:41:16 2020\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's multi_logloss: 0.138529\tvalid_1's multi_logloss: 0.148645\n",
      "[200]\ttraining's multi_logloss: 0.0618359\tvalid_1's multi_logloss: 0.0804535\n",
      "[300]\ttraining's multi_logloss: 0.0497185\tvalid_1's multi_logloss: 0.0750165\n",
      "[400]\ttraining's multi_logloss: 0.0438903\tvalid_1's multi_logloss: 0.0743614\n",
      "Early stopping, best iteration is:\n",
      "[432]\ttraining's multi_logloss: 0.0424221\tvalid_1's multi_logloss: 0.0743461\n",
      "---> f1-score(macro) valid: 0.9418\n",
      "\n",
      "\n",
      "------------------\n",
      "- Fold 3/6 started at Tue May 19 22:08:10 2020\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's multi_logloss: 0.138588\tvalid_1's multi_logloss: 0.148307\n",
      "[200]\ttraining's multi_logloss: 0.0618549\tvalid_1's multi_logloss: 0.0803067\n",
      "[300]\ttraining's multi_logloss: 0.0497861\tvalid_1's multi_logloss: 0.074981\n",
      "[400]\ttraining's multi_logloss: 0.0439581\tvalid_1's multi_logloss: 0.0743513\n",
      "Early stopping, best iteration is:\n",
      "[437]\ttraining's multi_logloss: 0.0422859\tvalid_1's multi_logloss: 0.0743413\n",
      "---> f1-score(macro) valid: 0.9427\n",
      "\n",
      "\n",
      "------------------\n",
      "- Fold 4/6 started at Tue May 19 22:35:01 2020\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's multi_logloss: 0.138634\tvalid_1's multi_logloss: 0.147867\n",
      "[200]\ttraining's multi_logloss: 0.0618985\tvalid_1's multi_logloss: 0.0798058\n",
      "[300]\ttraining's multi_logloss: 0.0498369\tvalid_1's multi_logloss: 0.0744452\n",
      "[400]\ttraining's multi_logloss: 0.0440492\tvalid_1's multi_logloss: 0.0738069\n",
      "Early stopping, best iteration is:\n",
      "[424]\ttraining's multi_logloss: 0.0429399\tvalid_1's multi_logloss: 0.0737979\n",
      "---> f1-score(macro) valid: 0.9428\n",
      "\n",
      "\n",
      "------------------\n",
      "- Fold 5/6 started at Tue May 19 22:59:34 2020\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's multi_logloss: 0.138522\tvalid_1's multi_logloss: 0.148307\n",
      "[200]\ttraining's multi_logloss: 0.0618103\tvalid_1's multi_logloss: 0.0802331\n",
      "[300]\ttraining's multi_logloss: 0.0497278\tvalid_1's multi_logloss: 0.0748524\n",
      "[400]\ttraining's multi_logloss: 0.0439101\tvalid_1's multi_logloss: 0.0741964\n",
      "Early stopping, best iteration is:\n",
      "[433]\ttraining's multi_logloss: 0.042412\tvalid_1's multi_logloss: 0.074187\n",
      "---> f1-score(macro) valid: 0.9432\n",
      "\n",
      "\n",
      "------------------\n",
      "- Fold 6/6 started at Tue May 19 23:22:49 2020\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's multi_logloss: 0.138654\tvalid_1's multi_logloss: 0.147751\n",
      "[200]\ttraining's multi_logloss: 0.0619322\tvalid_1's multi_logloss: 0.079669\n",
      "[300]\ttraining's multi_logloss: 0.0498301\tvalid_1's multi_logloss: 0.0742962\n",
      "[400]\ttraining's multi_logloss: 0.0439953\tvalid_1's multi_logloss: 0.0736587\n",
      "Early stopping, best iteration is:\n",
      "[417]\ttraining's multi_logloss: 0.0432024\tvalid_1's multi_logloss: 0.0736407\n",
      "---> f1-score(macro) valid: 0.9429\n",
      "\n",
      "====== finish ======\n",
      "score list: [0.9425980644706634, 0.9417602182119512, 0.9427210749686828, 0.942841069641306, 0.9432477947453919, 0.9428544815151912]\n",
      "CV mean score(f1_macro): 0.9427, std: 0.0005\n",
      "oof score(f1_macro): 0.9427\n",
      "\n",
      "CPU times: user 3d 3h 44min 41s, sys: 13min 22s, total: 3d 3h 58min 4s\n",
      "Wall time: 2h 34min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_pred, test_proba, oof, oof_proba, model_name = train_lgbm_clf(_X, _y, _X_te, lgbm_params,\n",
    "                                                                   n_fold=N_FOLD,\n",
    "                                                                   verbose=VERBOSE,\n",
    "                                                                   random_state=KFOLD_SEED,\n",
    "                                                                   early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                                                                   show_fig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save path: ./../data/output/submission_nb091_LGBMClassifier_cv_0.9427.csv\n"
     ]
    }
   ],
   "source": [
    "save_path = f'{DIR_OUTPUT}submission_nb{NB}_{model_name}_cv_{f1_macro(y, oof):.4f}.csv'\n",
    "sub = pd.read_csv(PATH_SMPLE_SUB)\n",
    "# sub['open_channels'] = test_pred\n",
    "sub['open_channels'] = test_pred.astype(int)\n",
    "print(f'save path: {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(save_path, index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "oof proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save path: ./../data/output_ignore/probas_nb091_LGBMClassifier_cv_0.9427\n"
     ]
    }
   ],
   "source": [
    "save_path = f'{DIR_OUTPUT_IGNORE}probas_nb{NB}_{model_name}_cv_{f1_macro(y, oof):.4f}'\n",
    "print(f'save path: {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(save_path, oof_proba, test_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "処理のしやすさのために、バッチ番号を振る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_list = []\n",
    "for n in range(10):\n",
    "    batchs = np.ones(500000)*n\n",
    "    batch_list.append(batchs.astype(int))\n",
    "batch_list = np.hstack(batch_list)\n",
    "_X['batch'] = batch_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "group 特徴量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group 特徴量を作成\n",
    "group = group_feat_train(_X)\n",
    "_X = pd.concat([_X, group], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_score(0): 0.999662\n",
      "group_score(1): 0.987247\n",
      "group_score(2): 0.979590\n",
      "group_score(3): 0.997853\n",
      "group_score(4): 0.894205\n"
     ]
    }
   ],
   "source": [
    "for group in sorted(_X['group'].unique()):\n",
    "    idxs = _X['group'] == group\n",
    "    oof_grp = oof[idxs].astype(int)\n",
    "    y_grp = y[idxs]\n",
    "    print(f'group_score({group}): {f1_score(y_grp, oof_grp, average=\"micro\"):4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_idx = np.arange(len(_X))\n",
    "idxs = y != oof\n",
    "\n",
    "failed = np.zeros(len(_X))\n",
    "failed[idxs] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "b = np.ones(n)/n\n",
    "failed_move = np.convolve(failed, b, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'signal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'signal'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-6f46e55fe9d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi_gr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'group'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'group'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'signal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_gr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'group={group}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m500000\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'signal'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIkAAAFpCAYAAAAYznh9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAHXlJREFUeJzt3V+opeddL/DvtxmjUGsLzgiSmZqA01PnVKF1E3rohYX2yCQXMxf+IQNFK6FzcyL+KUJEqRKvajkKQvwzYqkWbIy9kA2O5EIjBTElu1RDkxLZRG0mChlrzE1pYzy/c7GXh93tJHt1Zq29z+z5fGDDep/3We/7u/mx9v7u531WZyYAAAAA3NrecNgFAAAAAHD4hEQAAAAACIkAAAAAEBIBAAAAECERAAAAABESAQAAAJAlQqK2H2/7YtsvvMb5tv2Nttttn2r7rtWXCQAAAMA6LbOS6BNJzr7O+XuSnF78XEzyWzdeFgAAAAAHad+QaGY+k+RfX2fK+SR/MDueSPKWtt+5qgIBAAAAWL9V7El0R5Lndx1fWYwBAAAAcJM4dpA3a3sxO4+k5Y1vfOP3v/3tbz/I2wMAAAAcaZ/73Of+ZWZOXM97VxESvZDk1K7jk4ux/2JmLiW5lCQbGxuztbW1gtsDAAAAkCRt//F637uKx802k/zY4lvO3p3k5Zn55xVcFwAAAIADsu9KorafSvLeJMfbXknyS0m+KUlm5reTXE5yb5LtJF9J8hPrKhYAAACA9dg3JJqZC/ucnyT/a2UVAQAAAHDgVvG4GQAAAAA3OSERAAAAAEIiAAAAAIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQJYMidqebfts2+22D17j/FvbPt72822fanvv6ksFAAAAYF32DYna3pbk4ST3JDmT5ELbM3um/WKSR2fmnUnuS/Kbqy4UAAAAgPVZZiXR3Um2Z+a5mXklySNJzu+ZM0m+bfH6zUn+aXUlAgAAALBuy4REdyR5ftfxlcXYbr+c5ANtryS5nOQnr3WhthfbbrXdunr16nWUCwAAAMA6rGrj6gtJPjEzJ5Pcm+STbf/LtWfm0sxszMzGiRMnVnRrAAAAAG7UMiHRC0lO7To+uRjb7f4kjybJzPx1km9JcnwVBQIAAACwfsuERE8mOd32rra3Z2dj6s09c76U5H1J0vZ7shMSeZ4MAAAA4Caxb0g0M68meSDJY0m+mJ1vMXu67UNtzy2mfTjJh9r+bZJPJfngzMy6igYAAABgtY4tM2lmLmdnQ+rdYx/Z9fqZJO9ZbWkAAAAAHJRVbVwNAAAAwE1MSAQAAACAkAgAAAAAIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAACQJUOitmfbPtt2u+2DrzHnR9s+0/bptn+42jIBAAAAWKdj+01oe1uSh5P8zyRXkjzZdnNmntk153SSn0/ynpl5qe13rKtgAAAAAFZvmZVEdyfZnpnnZuaVJI8kOb9nzoeSPDwzLyXJzLy42jIBAAAAWKdlQqI7kjy/6/jKYmy3tyV5W9u/avtE27PXulDbi2232m5dvXr1+ioGAAAAYOVWtXH1sSSnk7w3yYUkv9v2LXsnzcylmdmYmY0TJ06s6NYAAAAA3KhlQqIXkpzadXxyMbbblSSbM/PvM/P3Sf4uO6ERAAAAADeBZUKiJ5OcbntX29uT3Jdkc8+cP8nOKqK0PZ6dx8+eW2GdAAAAAKzRviHRzLya5IEkjyX5YpJHZ+bptg+1PbeY9liSL7d9JsnjSX5uZr68rqIBAAAAWK3OzKHceGNjY7a2tg7l3gAAAABHUdvPzczG9bx3VRtXAwAAAHATExIBAAAAICQCAAAAQEgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAlgyJ2p5t+2zb7bYPvs68H2o7bTdWVyIAAAAA67ZvSNT2tiQPJ7knyZkkF9qeuca8NyX5qSSfXXWRAAAAAKzXMiuJ7k6yPTPPzcwrSR5Jcv4a834lyUeTfHWF9QEAAABwAJYJie5I8vyu4yuLsf+n7buSnJqZP11hbQAAAAAckBveuLrtG5L8WpIPLzH3YtuttltXr1690VsDAAAAsCLLhEQvJDm16/jkYuw/vSnJO5L8Zdt/SPLuJJvX2rx6Zi7NzMbMbJw4ceL6qwYAAABgpZYJiZ5McrrtXW1vT3Jfks3/PDkzL8/M8Zm5c2buTPJEknMzs7WWigEAAABYuX1Dopl5NckDSR5L8sUkj87M020fantu3QUCAAAAsH7Hlpk0M5eTXN4z9pHXmPveGy8LAAAAgIN0wxtXAwAAAHDzExIBAAAAICQCAAAAQEgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAZMmQqO3Zts+23W774DXO/2zbZ9o+1fbP237X6ksFAAAAYF32DYna3pbk4ST3JDmT5ELbM3umfT7Jxsx8X5JPJ/nVVRcKAAAAwPoss5Lo7iTbM/PczLyS5JEk53dPmJnHZ+Yri8MnkpxcbZkAAAAArNMyIdEdSZ7fdXxlMfZa7k/yZ9c60fZi2622W1evXl2+SgAAAADWaqUbV7f9QJKNJB+71vmZuTQzGzOzceLEiVXeGgAAAIAbcGyJOS8kObXr+ORi7Ou0fX+SX0jyAzPztdWUBwAAAMBBWGYl0ZNJTre9q+3tSe5Lsrl7Qtt3JvmdJOdm5sXVlwkAAADAOu0bEs3Mq0keSPJYki8meXRmnm77UNtzi2kfS/KtSf647d+03XyNywEAAADw/6FlHjfLzFxOcnnP2Ed2vX7/iusCAAAA4ACtdONqAAAAAG5OQiIAAAAAhEQAAAAACIkAAAAAiJAIAAAAgAiJAAAAAIiQCAAAAIAIiQAAAACIkAgAAACACIkAAAAAiJAIAAAAgAiJAAAAAIiQCAAAAIAIiQAAAACIkAgAAACACIkAAAAAiJAIAAAAgAiJAAAAAIiQCAAAAIAIiQAAAADIkiFR27Ntn2273fbBa5z/5rZ/tDj/2bZ3rrpQAAAAANZn35Co7W1JHk5yT5IzSS60PbNn2v1JXpqZ707y60k+uupCAQAAAFifZVYS3Z1ke2aem5lXkjyS5PyeOeeT/P7i9aeTvK9tV1cmAAAAAOu0TEh0R5Lndx1fWYxdc87MvJrk5STfvooCAQAAAFi/Ywd5s7YXk1xcHH6t7RcO8v5AkuR4kn857CLgFqT34PDoPzgceg8Ox3+73jcuExK9kOTUruOTi7FrzbnS9liSNyf58t4LzcylJJeSpO3WzGxcT9HA9dN7cDj0Hhwe/QeHQ+/B4Wi7db3vXeZxsyeTnG57V9vbk9yXZHPPnM0kP754/cNJ/mJm5nqLAgAAAOBg7buSaGZebftAkseS3Jbk4zPzdNuHkmzNzGaS30vyybbbSf41O0ESAAAAADeJpfYkmpnLSS7vGfvIrtdfTfIj3+C9L32D84HV0HtwOPQeHB79B4dD78HhuO7eq6fCAAAAAFhmTyIAAAAAjri1h0Rtz7Z9tu122wevcf6b2/7R4vxn29657prgVrBE7/1s22faPtX2z9t+12HUCUfNfr23a94PtZ22vvUFVmCZ3mv7o4vPvqfb/uFB1whH1RK/d7617eNtP7/43fPew6gTjpK2H2/7YtsvvMb5tv2NRV8+1fZdy1x3rSFR29uSPJzkniRnklxoe2bPtPuTvDQz353k15N8dJ01wa1gyd77fJKNmfm+JJ9O8qsHWyUcPUv2Xtq+KclPJfnswVYIR9Myvdf2dJKfT/KemfnvSX76wAuFI2jJz75fTPLozLwzO19y9JsHWyUcSZ9IcvZ1zt+T5PTi52KS31rmouteSXR3ku2ZeW5mXknySJLze+acT/L7i9efTvK+tl1zXXDU7dt7M/P4zHxlcfhEkpMHXCMcRct87iXJr2TnnyJfPcji4Ahbpvc+lOThmXkpSWbmxQOuEY6qZfpvknzb4vWbk/zTAdYHR9LMfCY73y7/Ws4n+YPZ8USSt7T9zv2uu+6Q6I4kz+86vrIYu+acmXk1yctJvn3NdcFRt0zv7XZ/kj9ba0Vwa9i39xZLfU/NzJ8eZGFwxC3zufe2JG9r+1dtn2j7ev99BZa3TP/9cpIPtL2SnW/N/smDKQ1uad/o34RJkmNrKwe4KbT9QJKNJD9w2LXAUdf2DUl+LckHD7kUuBUdy86S+/dmZ/XsZ9p+78z826FWBbeGC0k+MTP/u+3/SPLJtu+Ymf9z2IUBX2/dK4leSHJq1/HJxdg157Q9lp3lh19ec11w1C3Te2n7/iS/kOTczHztgGqDo2y/3ntTknck+cu2/5Dk3Uk2bV4NN2yZz70rSTZn5t9n5u+T/F12QiPgxizTf/cneTRJZuavk3xLkuMHUh3cupb6m3CvdYdETyY53fautrdnZ5OyzT1zNpP8+OL1Dyf5i5mZNdcFR92+vdf2nUl+JzsBkX0ZYDVet/dm5uWZOT4zd87MndnZD+zczGwdTrlwZCzzO+efZGcVUdoez87jZ88dZJFwRC3Tf19K8r4kafs92QmJrh5olXDr2UzyY4tvOXt3kpdn5p/3e9NaHzebmVfbPpDksSS3Jfn4zDzd9qEkWzOzmeT3srPccDs7my7dt86a4FawZO99LMm3JvnjxV7xX5qZc4dWNBwBS/YesGJL9t5jSX6w7TNJ/iPJz82M1etwg5bsvw8n+d22P5OdTaw/aGEA3Ji2n8rOPz+OL/b7+qUk35QkM/Pb2dn/694k20m+kuQnlrqu3gQAAABg3Y+bAQAAAHATEBIBAAAAICQCAAAAQEgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQJYIidp+vO2Lbb/wGufb9jfabrd9qu27Vl8mAAAAAOu0zEqiTyQ5+zrn70lyevFzMclv3XhZAAAAABykfUOimflMkn99nSnnk/zB7HgiyVvafueqCgQAAABg/VaxJ9EdSZ7fdXxlMQYAAADATeLYQd6s7cXsPJKWN77xjd//9re//SBvDwAAAHCkfe5zn/uXmTlxPe9dRUj0QpJTu45PLsb+i5m5lORSkmxsbMzW1tYKbg8AAABAkrT9x+t97yoeN9tM8mOLbzl7d5KXZ+afV3BdAAAAAA7IviuJ2n4qyXuTHG97JckvJfmmJJmZ305yOcm9SbaTfCXJT6yrWAAAAADWY9+QaGYu7HN+kvyvlVUEAAAAwIFbxeNmAAAAANzkhEQAAAAACIkAAAAAEBIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAWTIkanu27bNtt9s+eI3zb237eNvPt32q7b2rLxUAAACAddk3JGp7W5KHk9yT5EySC23P7Jn2i0kenZl3JrkvyW+uulAAAAAA1meZlUR3J9memedm5pUkjyQ5v2fOJPm2xes3J/mn1ZUIAAAAwLotExLdkeT5XcdXFmO7/XKSD7S9kuRykp+81oXaXmy71Xbr6tWr11EuAAAAAOuwqo2rLyT5xMycTHJvkk+2/S/XnplLM7MxMxsnTpxY0a0BAAAAuFHLhEQvJDm16/jkYmy3+5M8miQz89dJviXJ8VUUCAAAAMD6LRMSPZnkdNu72t6enY2pN/fM+VKS9yVJ2+/JTkjkeTIAAACAm8S+IdHMvJrkgSSPJflidr7F7Om2D7U9t5j24SQfavu3ST6V5IMzM+sqGgAAAIDVOrbMpJm5nJ0NqXePfWTX62eSvGe1pQEAAABwUFa1cTUAAAAANzEhEQAAAABCIgAAAACERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAGTJkKjt2bbPtt1u++BrzPnRts+0fbrtH662TAAAAADW6dh+E9reluThJP8zyZUkT7bdnJlnds05neTnk7xnZl5q+x3rKhgAAACA1VtmJdHdSbZn5rmZeSXJI0nO75nzoSQPz8xLSTIzL662TAAAAADWaZmQ6I4kz+86vrIY2+1tSd7W9q/aPtH27KoKBAAAAGD99n3c7Bu4zukk701yMsln2n7vzPzb7kltLya5mCRvfetbV3RrAAAAAG7UMiuJXkhyatfxycXYbleSbM7Mv8/M3yf5u+yERl9nZi7NzMbMbJw4ceJ6awYAAABgxZYJiZ5McrrtXW1vT3Jfks09c/4kO6uI0vZ4dh4/e26FdQIAAACwRvuGRDPzapIHkjyW5ItJHp2Zp9s+1PbcYtpjSb7c9pkkjyf5uZn58rqKBgAAAGC1OjOHcuONjY3Z2to6lHsDAAAAHEVtPzczG9fz3mUeNwMAAADgiBMSAQAAACAkAgAAAEBIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAGTJkKjt2bbPtt1u++DrzPuhttN2Y3UlAgAAALBu+4ZEbW9L8nCSe5KcSXKh7ZlrzHtTkp9K8tlVFwkAAADAei2zkujuJNsz89zMvJLkkSTnrzHvV5J8NMlXV1gfAAAAAAdgmZDojiTP7zq+shj7f9q+K8mpmfnT17tQ24ttt9puXb169RsuFgAAAID1uOGNq9u+IcmvJfnwfnNn5tLMbMzMxokTJ2701gAAAACsyDIh0QtJTu06PrkY+09vSvKOJH/Z9h+SvDvJps2rAQAAAG4ey4RETyY53fautrcnuS/J5n+enJmXZ+b4zNw5M3cmeSLJuZnZWkvFAAAAAKzcviHRzLya5IEkjyX5YpJHZ+bptg+1PbfuAgEAAABYv2PLTJqZy0ku7xn7yGvMfe+NlwUAAADAQbrhjasBAAAAuPkJiQAAAAAQEgEAAAAgJAIAAAAgQiIAAAAAIiQCAAAAIEIiAAAAACIkAgAAACBCIgAAAAAiJAIAAAAgQiIAAAAAIiQCAAAAIEIiAAAAACIkAgAAACBCIgAAAAAiJAIAAAAgQiIAAAAAIiQCAAAAIEIiAAAAACIkAgAAACBCIgAAAACyZEjU9mzbZ9tut33wGud/tu0zbZ9q++dtv2v1pQIAAACwLvuGRG1vS/JwknuSnElyoe2ZPdM+n2RjZr4vyaeT/OqqCwUAAABgfZZZSXR3ku2ZeW5mXknySJLzuyfMzOMz85XF4RNJTq62TAAAAADWaZmQ6I4kz+86vrIYey33J/mza51oe7HtVtutq1evLl8lAAAAAGu10o2r234gyUaSj13r/MxcmpmNmdk4ceLEKm8NAAAAwA04tsScF5Kc2nV8cjH2ddq+P8kvJPmBmfnaasoDAAAA4CAss5LoySSn297V9vYk9yXZ3D2h7TuT/E6SczPz4urLBAAAAGCd9g2JZubVJA8keSzJF5M8OjNPt32o7bnFtI8l+dYkf9z2b9puvsblAAAAAPj/0DKPm2VmLie5vGfsI7tev3/FdQEAAABwgFa6cTUAAAAANychEQAAAABCIgAAAACERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAERIBAAAAECERAAAAABESAQAAABAhEQAAAAAREgEAAAAQIREAAAAAGTJkKjt2bbPtt1u++A1zn9z2z9anP9s2ztXXSgAAAAA67NvSNT2tiQPJ7knyZkkF9qe2TPt/iQvzcx3J/n1JB9ddaEAAAAArM8yK4nuTrI9M8/NzCtJHklyfs+c80l+f/H600ne17arKxMAAACAdVomJLojyfO7jq8sxq45Z2ZeTfJykm9fRYEAAAAArN+xg7xZ24tJLi4Ov9b2Cwd5fyBJcjzJvxx2EXAL0ntwePQfHA69B4fjv13vG5cJiV5IcmrX8cnF2LXmXGl7LMmbk3x574Vm5lKSS0nSdmtmNq6naOD66T04HHoPDo/+g8Oh9+BwtN263vcu87jZk0lOt72r7e1J7kuyuWfOZpIfX7z+4SR/MTNzvUUBAAAAcLD2XUk0M6+2fSDJY0luS/LxmXm67UNJtmZmM8nvJflk2+0k/5qdIAkAAACAm8RSexLNzOUkl/eMfWTX668m+ZFv8N6XvsH5wGroPTgceg8Oj/6Dw6H34HBcd+/VU2EAAAAALLMnEQAAAABH3NpDorZn2z7bdrvtg9c4/81t/2hx/rNt71x3TXArWKL3frbtM22favvnbb/rMOqEo2a/3ts174faTlvf+gIrsEzvtf3RxWff023/8KBrhKNqid8739r28bafX/zuee9h1AlHSduPt32x7Rde43zb/saiL59q+65lrrvWkKjtbUkeTnJPkjNJLrQ9s2fa/UlempnvTvLrST66zprgVrBk730+ycbMfF+STyf51YOtEo6eJXsvbd+U5KeSfPZgK4SjaZnea3s6yc8nec/M/PckP33ghcIRtORn3y8meXRm3pmdLzn6zYOtEo6kTyQ5+zrn70lyevFzMclvLXPRda8kujvJ9sw8NzOvJHkkyfk9c84n+f3F608neV/brrkuOOr27b2ZeXxmvrI4fCLJyQOuEY6iZT73kuRXsvNPka8eZHFwhC3Tex9K8vDMvJQkM/PiAdcIR9Uy/TdJvm3x+s1J/ukA64MjaWY+k51vl38t55P8wex4Islb2n7nftddd0h0R5Lndx1fWYxdc87MvJrk5STfvua64Khbpvd2uz/Jn621Irg17Nt7i6W+p2bmTw+yMDjilvnce1uSt7X9q7ZPtH29/74Cy1um/345yQfaXsnOt2b/5MGUBre0b/RvwiTJsbWVA9wU2n4gyUaSHzjsWuCoa/uGJL+W5IOHXArcio5lZ8n9e7OzevYzbb93Zv7tUKuCW8OFJJ+Ymf/d9n8k+WTbd8zM/znswoCvt+6VRC8kObXr+ORi7Jpz2h7LzvLDL6+5Ljjqlum9tH1/kl9Icm5mvnZAtcFRtl/vvSnJO5L8Zdt/SPLuJJs2r4Ybtszn3pUkmzPz7zPz90n+LjuhEXBjlum/+5M8miQz89dJviXJ8QOpDm5dS/1NuNe6Q6Ink5xue1fb27OzSdnmnjmbSX588fqHk/zFzMya64Kjbt/ea/vOJL+TnYDIvgywGq/bezPz8swcn5k7Z+bO7OwHdm5mtg6nXDgylvmd80+ys4oobY9n5/Gz5w6ySDiilum/LyV5X5K0/Z7shERXD7RKuPVsJvmxxbecvTvJyzPzz/u9aa2Pm83Mq20fSPJYktuSfHxmnm77UJKtmdlM8nvZWW64nZ1Nl+5bZ01wK1iy9z6W5FuT/PFir/gvzcy5QysajoAlew9YsSV777EkP9j2mST/keTnZsbqdbhBS/bfh5P8btufyc4m1h+0MABuTNtPZeefH8cX+339UpJvSpKZ+e3s7P91b5LtJF9J8hNLXVdvAgAAALDux80AAAAAuAkIiQAAAAAQEgEAAAAgJAIAAAAgQiIAAAAAIiQCAAAAIEIiAAAAACIkAgAAACDJ/wVpCVxbySwAbAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(20, 6))\n",
    "axs = axs.ravel()\n",
    "# fig = plt.figure(figsize=(20, 3))\n",
    "\n",
    "for i_gr, group in enumerate(sorted(_X['group'].unique())):\n",
    "    idxs = _X['group'] == group\n",
    "    axs[0].plot(np.arange(len(_X))[idxs], _X['signal'].values[idxs], color=cp[i_gr], label=f'group={group}')\n",
    "for x in range(10): \n",
    "    axs[0].axvline(x*500000 + 500000, color='gray') \n",
    "    axs[0].text(x*500000 + 250000, 0.6, x)\n",
    "axs[0].plot(x_idx, failed_move, '.', color='black', label='failed_mv')\n",
    "axs[0].set_xlim(0, 5500000)\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(x_idx, y)\n",
    "axs[1].set_xlim(0, 5500000)\n",
    "\n",
    "# fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
